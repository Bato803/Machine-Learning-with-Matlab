## Introduction

In this mini-project, we will be using support vector machines (SVMs) to build a spam classifier, but before that we will explore
support vector machine with various example 2D datasets to gain an intuition of how SVMs work and how to use a Gaussian kernel
with SVMs. 

## How outlier affect decision boundary
In this part we will train a linear SVM on a 2D dataset. The visualization of dataset goes as follow, notice that there is an outlier
positive exmaple on the far left at about (0.1, 4.1)

<img src="https://user-images.githubusercontent.com/17235054/31854844-b8167644-b66d-11e7-8638-9cf817b12ee9.jpg" width="400" height="300">


We use different penalty parameter *C* for misclassified training examples. A large C parameter tends to tell the SVMs to 
get every example correct. So *C* plays a role similar to the reciprocal of regularization degree. The larger the C, the less
regularization, and the model is more likely to overfit the data. Different clssification boundaries coresponding to *C=1* and *C=100* 
goes as follows:

<img src="https://user-images.githubusercontent.com/17235054/31854935-c073c82c-b66e-11e7-8b02-1984b2771e9f.jpg" width=400 height=300>

<img src="https://user-images.githubusercontent.com/17235054/31854936-c07f332e-b66e-11e7-9611-89c26dfad3e5.jpg" width=400 height=300>


## Non-linear Classification with Gaussian kernel
The dataset we will be using in this part is non-linear separable, and it's plot below:

<img src="https://user-images.githubusercontent.com/17235054/31854995-9f016e64-b66f-11e7-95e3-5b69b71b7cfb.jpg" width=400 height=300>

To find the non-linar decision boundary, we need to implement a Gaussian kernel. This kernel measures the "distance" between 
a pair of examples, and it's parameterized by bandwith sigma. This parameter determines how fast the similarity metric decrease
to zero as the example are further apart. The Gaussian function is defined as:

<a href="https://www.codecogs.com/eqnedit.php?latex=K_{gaussian}(x^{(i)},&space;x^{(j)})&space;=&space;exp(-\frac{||x^{(i)}&space;-&space;x^{(j)}||^2}{2\sigma^2})=exp(-\frac{\sum^n_{k=1}(x^{(i)}_k-x^{(j)}_k)^2}{2\sigma^2})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K_{gaussian}(x^{(i)},&space;x^{(j)})&space;=&space;exp(-\frac{||x^{(i)}&space;-&space;x^{(j)}||^2}{2\sigma^2})=exp(-\frac{\sum^n_{k=1}(x^{(i)}_k-x^{(j)}_k)^2}{2\sigma^2})" title="K_{gaussian}(x^{(i)}, x^{(j)}) = exp(-\frac{||x^{(i)} - x^{(j)}||^2}{2\sigma^2})=exp(-\frac{\sum^n_{k=1}(x^{(i)}_k-x^{(j)}_k)^2}{2\sigma^2})" /></a>

After training, the decision boundary generated by this model is:

<img src="https://user-images.githubusercontent.com/17235054/31855246-75e1166e-b675-11e7-9fdd-7170415cc1bc.jpg" width=400 height=300>


## Find the best *C* and *Sigma* for SVM with Gaussian Kernel
This time we will use the cross validation set to determin the best *C* and *Sigma* to use, for both *C* and *sigma*, we search through
every combination of `(0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30`). After searching, we found that the best *C* and *Sigma* is 1 and 0.1
The result goes as follows:

<img src="https://user-images.githubusercontent.com/17235054/31855322-e5f419fa-b676-11e7-8a29-0d9fc070f478.jpg" width=400 height=300>


## Spam Classification - Preprocessing Email
